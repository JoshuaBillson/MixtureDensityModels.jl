var documenterSearchIndex = {"docs":
[{"location":"mlj/","page":"MLJ Compatibility","title":"MLJ Compatibility","text":"CurrentModule = MixtureDensityNetworks","category":"page"},{"location":"mlj/#MLJ-Compatibility","page":"MLJ Compatibility","title":"MLJ Compatibility","text":"","category":"section"},{"location":"mlj/","page":"MLJ Compatibility","title":"MLJ Compatibility","text":"This package implements the interface specified by MLJModelInterface and is thus fully compatible with the MLJ ecosystem. Below is an example demonstrating the use of this package in conjunction with MLJ. ","category":"page"},{"location":"mlj/","page":"MLJ Compatibility","title":"MLJ Compatibility","text":"using MixtureDensityNetworks, Distributions, Logging, TerminalLoggers, CairoMakie, MLJ\n\nconst n_samples = 1000\nconst epochs = 1000\nconst mixtures = 6\nconst layers = [128, 128]\n\nfunction main()\n    # Generate Data\n    X, Y = generate_data(n_samples)\n\n    # Create Model\n    mach = MLJ.machine(MDN(epochs=epochs, mixtures=mixtures, layers=layers), MLJ.table(X'), Y[1,:])\n\n    # Evaluate Model\n    with_logger(TerminalLogger()) do \n        @info \"Evaluating...\"\n        evaluation = MLJ.evaluate!(\n            mach, \n            resampling=Holdout(shuffle=true), \n            measure=[rsq, rmse, mae, mape], \n            operation=MLJ.predict_mean\n        )\n        names = [\"R²\", \"RMSE\", \"MAE\", \"MAPE\"]\n        metrics = round.(evaluation.measurement, digits=3)\n        @info \"Metrics: \" * join([\"$name: $metric\" for (name, metric) in zip(names, metrics)], \", \")\n    end\n\n    # Fit Model\n    with_logger(TerminalLogger()) do \n        @info \"Training...\"\n        MLJ.fit!(mach)\n    end\n\n    # Plot Learning Curve\n    fig, _, _ = lines(1:epochs, MLJ.training_losses(mach), axis=(;xlabel=\"Epochs\", ylabel=\"Loss\"))\n    save(\"LearningCurve.png\", fig)\n\n    # Plot Learned Distribution\n    Ŷ = MLJ.predict(mach) .|> rand\n    fig, ax, plt = scatter(X[1,:], Ŷ, markersize=4, label=\"Predicted Distribution\")\n    scatter!(ax, X[1,:], Y[1,:], markersize=3, label=\"True Distribution\")\n    axislegend(ax, position=:lt)\n    save(\"PredictedDistribution.png\", fig)\n\n    # Plot Conditional Distribution\n    cond = MLJ.predict(mach, MLJ.table(reshape([-2.1], (1,1))))[1]\n    fig = Figure(resolution=(1000, 500))\n    density(fig[1,1], rand(cond, 10000), npoints=10000)\n    save(\"ConditionalDistribution.png\", fig)\nend\n\nmain()","category":"page"},{"location":"reference/","page":"API (Reference Manual)","title":"API (Reference Manual)","text":"CurrentModule = MixtureDensityNetworks","category":"page"},{"location":"reference/#Index","page":"API (Reference Manual)","title":"Index","text":"","category":"section"},{"location":"reference/","page":"API (Reference Manual)","title":"API (Reference Manual)","text":"","category":"page"},{"location":"reference/#API","page":"API (Reference Manual)","title":"API","text":"","category":"section"},{"location":"reference/","page":"API (Reference Manual)","title":"API (Reference Manual)","text":"Modules = [MixtureDensityNetworks]\nPrivate = false","category":"page"},{"location":"reference/#MixtureDensityNetworks.MDN","page":"API (Reference Manual)","title":"MixtureDensityNetworks.MDN","text":"MDN\n\nA model type for constructing a MDN, based on MixtureDensityNetworks.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nMDN = @load MDN pkg=MixtureDensityNetworks\n\nDo model = MDN() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in MDN(mixtures=...).\n\nA neural network which parameterizes a Gaussian Mixture Model (GMM)  distributed over the target varible y conditioned on the features X.\n\nTraining Data\n\nIn MLJ or MLJBase, bind an MDN instance model to data with     mach = machine(model, X, y) where\n\nX: any table of input features (eg, a DataFrame) whose columns belong to the Continuous scitypes`.\ny: the target, which can be any AbstractVector whose element scitype is Continuous.\n\nHyperparameters\n\nmixtures=5: number of Gaussian mixtures in the predicted distribution\nlayers=[128,]: hidden layer topology, starting from the first hidden layer\nη=1e-3: learning rate used for the optimizer\nepochs=1: number of epochs to train the model\nbatchsize=32: batch size used during training\n\nOperations\n\npredict(mach, Xnew): return the distributions over the target conditioned on the new features Xnew having the same scitype as X above.\npredict_mode(mach, Xnew): return the largest modes of the distributions over targets   conditioned on the new features Xnew having the same scitype as X above.\npredict_mean(mach, Xnew): return the means of the distributions over targets   conditioned on the new features Xnew having the same scitype as X above.\npredict_median(mach, Xnew): return the medians of the distributions over targets   conditioned on the new features Xnew having the same scitype as X above.\n\nFitted Parameters\n\nThe fields of fitted_params(mach) are:\n\nfitresult: the trained mixture density model, compatible with the Flux ecosystem.\n\nReport\n\nlearning_curve: the average training loss for each epoch.\nbest_epoch: the epoch (starting from 1) with the lowest training loss.\nbest_loss: the best (lowest) loss encountered durind training. Corresponds  to the average loss of the best epoch.\n\nAccessor Functions\n\ntraining_losses(mach) returns the learning curve as a vector of average  training losses for each epoch.\n\nExamples\n\nusing MLJ\nMDN = @load MDN pkg=MixtureDensityNetworks\nmdn = MDN(mixtures=12, epochs=100, layers=[512, 256, 128])\nX, y = make_regression(100, 1) # synthetic data\nmach = machine(mdn, X, y) |> fit!\nXnew, _ = make_regression(3, 1)\nŷ = predict(mach, Xnew) # new predictions\nreport(mach).best_epoch # best epoch encountered during training \nreport(mach).best_loss # best loss encountered during training \ntraining_losses(mach) # learning curve\n\n\n\n\n\n","category":"type"},{"location":"reference/#MixtureDensityNetworks.MDN-Tuple{}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.MDN","text":"MDN(; mixtures=5, layers=[128], η=1e-3, epochs=1, batchsize=32)\n\nDefines an MDN model with the given hyperparameters.\n\nParameters\n\nmixtures: The number of gaussian mixtures to use in estimating the conditional distribution (default=5).\nlayers: A vector indicating the number of nodes in each of the hidden layers (default=[128,]).\nη: The learning rate to use when training the model (default=1e-3).\nepochs: The number of epochs to train the model (default=1).\nbatchsize: The batchsize to use during training (default=32).\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.Machine","page":"API (Reference Manual)","title":"MixtureDensityNetworks.Machine","text":"mutable struct Machine\n\nA struct for associating a model's hyperparameters with its training state.\n\nParameters\n\nhypers::MDN\nfitresult::Union{Nothing, MixtureDensityNetwork}\nreport::NamedTuple{(:learning_curve, :best_epoch, :best_loss), Tuple{Vector{Float64}, Int64, Float64}}\n\n\n\n\n\n","category":"type"},{"location":"reference/#MixtureDensityNetworks.Machine-Tuple{MDN}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.Machine","text":"Machine(hypers::MDN)\n\nBinds a collection of hyperparameters with a training state for fitting, evaluating, and predicting.\n\nParameters\n\nhypers: The hyperparameters we want our model to conform to.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.MixtureDensityNetwork","page":"API (Reference Manual)","title":"MixtureDensityNetworks.MixtureDensityNetwork","text":"struct MixtureDensityNetwork\n\nA custom Flux model whose predictions paramaterize a Gaussian Mixture Model.\n\nParameters\n\nhidden::Flux.Chain\nμ::Flux.Dense\nΣ::Flux.Dense\nπ::Flux.Chain\n\n\n\n\n\n","category":"type"},{"location":"reference/#MixtureDensityNetworks.MixtureDensityNetwork-Tuple{Int64, Vector{Int64}, Int64}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.MixtureDensityNetwork","text":"MixtureDensityNetwork(\n    input::Int64,\n    layers::Vector{Int64},\n    mixtures::Int64\n) -> MixtureDensityNetwork\n\n\nConstruct a new MixtureDensityNetwork.\n\nParameters\n\ninput: The dimension of the input features.\nlayers: The topolgy of the hidden layers, starting from the first layer.\nmixtures: The number of Gaussian mixtures to use in the conditional distribution.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.fit!-Tuple{Machine, Matrix{<:Real}, Matrix{<:Real}}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.fit!","text":"fit!(\n    machine::Machine,\n    X::Matrix{<:Real},\n    Y::Matrix{<:Real}\n) -> NamedTuple{(:learning_curve, :best_epoch, :best_loss), Tuple{Vector{Float64}, Int64, Float64}}\n\n\nFit the model to the data given by X and Y.\n\nParameters\n\nmachine: The machine to be trained.\nX: A dxn matrix where d is the number of features and n is the number of samples.\nY: A 1xn matrix where n is the number of samples.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.generate_data-Tuple{Int64}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.generate_data","text":"generate_data(\n    n_samples::Int64\n) -> Tuple{Matrix{Float64}, Matrix{Float64}}\n\n\nGenerate some synthetic data for testing purposes. \n\nParameters\n\nn_samples: The number of samples we want to generate.\n\nReturns\n\nThe sythetic features X and labels Y as a tuple (X, Y).\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.likelihood_loss-Tuple{Matrix{<:Real}, Matrix{<:Real}, Matrix{<:Real}, Matrix{<:Real}}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.likelihood_loss","text":"likelihood_loss(\n    μ::Matrix{<:Real},\n    σ::Matrix{<:Real},\n    pi::Matrix{<:Real},\n    y::Matrix{<:Real}\n) -> Float64\n\n\nConpute the negative log-likelihood loss for a set of labels y under a Gaussian Mixture Model defined by the parameters μ, σ, and pi.\n\nParameters\n\nμ: A mxn matrix of means where m is the number of Gaussian mixtures and n is the number of samples.\nσ: A mxn matrix of standard deviations where m is the number of Gaussian mixtures and n is the number of samples.\npi: A mxn matrix of priors where m is the number of Gaussian mixtures and n is the number of samples.\ny: A 1xn matrix of labels where n is the number of samples.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.predict-Tuple{Machine, Matrix{<:Real}}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.predict","text":"predict(\n    machine::Machine,\n    X::Matrix{<:Real}\n) -> Vector{Distributions.MixtureModel}\n\n\nPredict the full conditional distribution P(Y|X).\n\nParameters\n\nmachine: The machine with which we want to generate a prediction.\nX: A dxn matrix where d is the number of features and n is the number of samples.\n\nReturns\n\nReturns a vector of Distributions.MixtureModel objects representing the conditional distribution for each sample.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.predict_mean-Tuple{Machine, Matrix{<:Real}}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.predict_mean","text":"predict_mean(\n    machine::Machine,\n    X::Matrix{<:Real}\n) -> AbstractVector\n\n\nPredict the mean of the conditional distribution P(Y|X). \n\nParameters\n\nmachine: The machine with which we want to generate a prediction.\nX: A dxn matrix where d is the number of features and n is the number of samples.\n\nReturns\n\nReturns a vector of real numbers representing the mean of the conditional distribution P(Y|X) for each sample.\n\n\n\n\n\n","category":"method"},{"location":"reference/#MixtureDensityNetworks.predict_mode-Tuple{Machine, Matrix{<:Real}}","page":"API (Reference Manual)","title":"MixtureDensityNetworks.predict_mode","text":"predict_mode(machine::Machine, X::Matrix{<:Real}) -> Any\n\n\nPredict the mean of the Gaussian with the largest prior in the conditional distribution P(Y|X). \n\nParameters\n\nmachine: The machine with which we want to generate a prediction.\nX: A dxn matrix where d is the number of features and n is the number of samples.\n\nReturns\n\nReturns a vector of real numbers representing the most significant mode for each sample.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Introduction","title":"Introduction","text":"CurrentModule = MixtureDensityNetworks","category":"page"},{"location":"#MixtureDensityNetworks","page":"Introduction","title":"MixtureDensityNetworks","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This package provides a simple interface for defining, training, and deploying Mixture Density Networks (MDNs). MDNs were first proposed by Bishop (1994). We can think of an MDN as a specialized type of Artificial Neural Network (ANN), which takes some features X and returns a distribution over the labels Y under a Gaussian Mixture Model (GMM). Unlike an ANN, MDNs maintain the full conditional distribution P(Y|X). This makes them particularly well-suited for situations where we want to maintain some measure of the uncertainty in our predictions. Moreover, because GMMs can represent multimodal distributions, MDNs are capable of modelling one-to-many relationships, which occurs when each input X can be associated with more than one output Y. ","category":"page"},{"location":"#Example","page":"Introduction","title":"Example","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"First, let's create our dataset. To properly demonstrate the power of MDNs, we'll generate a many-to-one dataset where each x-value can map to more than one y-value.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using Distributions, CairoMakie, MixtureDensityNetworks\n\nconst n_samples = 1000\n\nX, Y = generate_data(n_samples)\n\nfig, ax, plt = scatter(X[1,:], Y[1,:], markersize=5)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Now we'll define our model and training parameters. For this example, we construct a network with 2 hidden layers of size 128, 5 Gaussian  mixtures, and we train for 1000 epochs. All other hyperparameters are set to their default values.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"model = MDN(epochs=1000, mixtures=5, layers=[128, 128])","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"We can fit our model to our training data by calling fit!(model, X, Y). This method returns the learning curve, which we plot below.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"lc = fit!(model, X, Y)\nfig, _, _ = lines(1:1000, lc, axis=(;xlabel=\"Epochs\", ylabel=\"Loss\"))","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Let's evaluate how well our model learned to replicate our data by plotting both the learned and true distributions. We observe that our model has indeed learned to replicate the true distribution.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Ŷ = predict(model, X)\nfig, ax, plt = scatter(X[1,:], rand.(Ŷ), markersize=3, label=\"Predicted Distribution\")\nscatter!(ax, X[1,:], Y[1,:], markersize=3, label=\"True Distribution\")\naxislegend(ax, position=:lt)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"We can also visualize the conditional distribution predicted by our model at x = -2.0.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"cond = predict(model, reshape([-2.0], (1,1)))[1]\nfig = Figure(resolution=(1000, 500))\ndensity(fig[1,1], rand(cond, 10000), npoints=10000)","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: )","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Below is a script for running the complete example.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using MixtureDensityNetworks, Distributions, CairoMakie, Logging, TerminalLoggers\n\nconst n_samples = 1000\nconst epochs = 1000\nconst mixtures = 6\nconst layers = [128, 128]\n\nfunction main()\n    # Generate Data\n    X, Y = generate_data(n_samples)\n\n    # Create Model\n    machine = MixtureDensityNetworks.Machine(MDN(epochs=epochs, mixtures=mixtures, layers=layers))\n\n    # Fit Model\n    report = with_logger(TerminalLogger()) do \n        fit!(machine, X, Y)\n    end\n\n    # Plot Learning Curve\n    fig, _, _ = lines(1:epochs, report.learning_curve, axis=(;xlabel=\"Epochs\", ylabel=\"Loss\"))\n    save(\"LearningCurve.png\", fig)\n\n    # Plot Learned Distribution\n    Ŷ = predict(machine, X)\n    fig, ax, plt = scatter(X[1,:], rand.(Ŷ), markersize=4, label=\"Predicted Distribution\")\n    scatter!(ax, X[1,:], Y[1,:], markersize=3, label=\"True Distribution\")\n    axislegend(ax, position=:lt)\n    save(\"PredictedDistribution.png\", fig)\n\n    # Plot Conditional Distribution\n    cond = predict(machine, reshape([-2.0], (1,1)))[1]\n    fig = Figure(resolution=(1000, 500))\n    density(fig[1,1], rand(cond, 10000), npoints=10000)\n    save(\"ConditionalDistribution.png\", fig)\nend\n\nmain()","category":"page"}]
}
